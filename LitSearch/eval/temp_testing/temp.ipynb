{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 22280.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BERT[SEP]We introduce a new language representation model called BERT', 'Attention is all you need[SEP]The dominant sequence transduction models are based on complex recurrent or convolutional neural networks']\n",
      "['BERT[SEP]We introduce a new language representation model called BERT', 'Attention is all you need[SEP]The dominant sequence transduction models are based on complex recurrent or convolutional neural networks']\n",
      "['BERT[SEP]We introduce a new language representation model called BERT', 'Attention is all you need[SEP]The dominant sequence transduction models are based on complex recurrent or convolutional neural networks']\n",
      "['BERT[SEP]We introduce a new language representation model called BERT', 'Attention is all you need[SEP]The dominant sequence transduction models are based on complex recurrent or convolutional neural networks']\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "from utils import utils\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "model = AutoAdapterModel.from_pretrained('allenai/specter2_base')\n",
    "model.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"specter2\", set_active=True)\n",
    "\n",
    "papers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\n",
    "          {'title': 'Attention is all you need', 'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'},\n",
    "          {'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\n",
    "          {'title': 'Attention is all you need', 'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'},\n",
    "          {'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\n",
    "          {'title': 'Attention is all you need', 'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'},\n",
    "          {'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\n",
    "          {'title': 'Attention is all you need', 'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'},]\n",
    "\n",
    "\n",
    "\n",
    "# concatenate title and abstract\n",
    "text_batch = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\n",
    "for data in utils.batch_iterator(text_batch, 2):\n",
    "    print(data)\n",
    "\n",
    "\n",
    "# preprocess the input\n",
    "inputs = tokenizer(text_batch, padding=True, truncation=True,\n",
    "                                   return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
    "output = model(**inputs)\n",
    "# take the first token in the batch as the embedding\n",
    "embeddings = output.last_hidden_state[:, 0, :]\n",
    "\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/hc387/miniconda3/envs/litsearch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertAdapterModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 39016.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1[SEP]1': 123, '2[SEP]2': 238, '3[SEP]3': 124, '4[SEP]4': 12095483}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "from utils import utils\n",
    "from eval.retrieval.kv_store import KVStore\n",
    "from eval.retrieval.kv_store import TextType\n",
    "from typing import List, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "model = AutoAdapterModel.from_pretrained('allenai/specter2_base')\n",
    "model.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"specter2\", set_active=True)\n",
    "\n",
    "def encode_batch(texts: List[str], type: TextType, show_progress_bar: bool = True, batch_size: int = 256) -> List[Any]:\n",
    "        encoded_keys = []\n",
    "        total_batches = utils.batch_size_calc(texts, batch_size)\n",
    "\n",
    "        iterator = utils.batch_iterator(texts, batch_size)\n",
    "        if show_progress_bar:\n",
    "            iterator = tqdm(iterator, total=total_batches, desc=\"Encoding Batches\")\n",
    "\n",
    "        for text_batch in iterator:\n",
    "            inputs = tokenizer(text_batch, \n",
    "                                        padding=True, \n",
    "                                        truncation=True,\n",
    "                                        return_tensors=\"pt\", \n",
    "                                        return_token_type_ids=False,\n",
    "                                        max_length=512)\n",
    "            output = model(**inputs)\n",
    "            embeddings = output.last_hidden_state[:, 0, :].detach().clone().requires_grad_(False)\n",
    "            encoded_keys.extend(embeddings)\n",
    "        return encoded_keys\n",
    "\n",
    "\n",
    "\n",
    "temp_text = [\n",
    "    {\"corpusid\": 123, \"title\": \"1\", \"abstract\": \"1\"},\n",
    "    {\"corpusid\": 238, \"title\": \"2\", \"abstract\": \"2\"},\n",
    "    {\"corpusid\": 124, \"title\": \"3\", \"abstract\": \"3\"},\n",
    "    {\"corpusid\": 12095483, \"title\": \"4\", \"abstract\": \"4\"}\n",
    "]\n",
    "\n",
    "kv_pairs = {utils.get_title_abstract_SEPtoken(record): utils.get_clean_corpusid(record) for record in temp_text}\n",
    "print(kv_pairs)\n",
    "\n",
    "\n",
    "\n",
    "# temp = encode_batch(temp_text, TextType.KEY)\n",
    "# print(len(temp))\n",
    "# for i in range(len(temp)):\n",
    "#      print(len(temp[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 48210.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "from utils import utils\n",
    "from eval.retrieval.specter2 import SPECTER2\n",
    "from eval.retrieval.build_index import create_index, create_kv_pairs\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "papers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT', 'corpusid': 123},\n",
    "          {'title': 'Attention is all you need', 'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks', 'corpusid': 2342354},\n",
    "          {'title': 'Situated Faithfulness', 'abstract': 'In this work, we explore whether LLM is faithful to the promot', 'corpusid': 1029310938}]\n",
    "\n",
    "papers.to(device)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--index_type\", required=True) # bm25, instructor, e5, gtr, grit\n",
    "parser.add_argument(\"--key\", required=True) # title_absract, full_paper, paragraphs\n",
    "parser.add_argument(\"--save_as_tensor\", required=True) # True or False \n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "specter2 = create_index(args)\n",
    "kv_pairs = create_kv_pairs(papers, args.key)\n",
    "specter2.create_index(kv_pairs)\n",
    "print(specter2.keys)\n",
    "print(specter2.values)\n",
    "specter2.save('/usr/xtmp/hc387/ai_reviewer/LitSearch/eval/temp_testing/temp_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.1265e-01,  5.3322e-01, -6.8603e-01, -6.2978e-01, -1.4235e-01,\n",
      "         2.6158e-02,  5.9945e-01, -1.2765e-01, -6.3341e-01,  1.5257e-01,\n",
      "         5.5645e-01,  9.4629e-02,  1.7317e-01, -2.0690e-01, -2.1159e-03,\n",
      "         3.3517e-01, -1.0051e+00,  7.3751e-01, -4.6397e-02, -4.0637e-01,\n",
      "        -5.1213e-01, -5.5537e-01, -1.0084e+00,  4.4403e-01, -4.1524e-02,\n",
      "         2.7774e-01,  2.8313e-01,  9.3435e-01, -3.6585e-01,  4.2380e-01,\n",
      "         2.1834e-01, -4.2578e-01,  5.8036e-02,  2.3727e-01, -2.2411e-01,\n",
      "        -4.7528e-01, -6.9942e-02, -5.4209e-01, -7.0977e-01,  3.4004e-01,\n",
      "        -2.8111e-01,  2.2354e-01,  4.2218e-01, -7.9238e-01,  1.2158e-01,\n",
      "         1.0396e+00,  6.5995e-01,  6.9057e-01,  1.3489e-01, -1.1797e+00,\n",
      "         1.4621e+00, -1.1961e+00,  2.1893e-01,  1.8246e+00,  8.8473e-02,\n",
      "         2.0602e-01, -4.4415e-01, -4.8353e-01,  3.6026e-01,  1.3830e-03,\n",
      "        -1.0849e+00, -4.4678e-02, -3.7058e-01, -3.6315e-02,  1.3344e+00,\n",
      "         1.7956e-01, -1.5120e-01,  4.9890e-01, -1.0016e-01,  1.4742e+00,\n",
      "         2.1212e-01, -9.3612e-01, -4.0819e-01,  2.3963e-01,  6.1216e-01,\n",
      "         7.1756e-01, -8.6811e-02,  7.1436e-01, -8.2428e-01, -7.1581e-02,\n",
      "         3.1938e-01,  1.3814e-01,  2.2346e-01, -2.8063e-01, -2.3896e-01,\n",
      "         7.9782e-01,  1.1648e-01,  9.7847e-01, -2.5420e-02,  1.0459e+00,\n",
      "         5.7735e-01,  3.1958e-01, -5.8523e-01,  1.4019e-02,  3.3406e-01,\n",
      "        -4.2651e-02, -3.7615e-01,  2.1832e-01,  3.0527e-01,  7.9311e-01,\n",
      "        -5.2962e-01,  2.8211e-01, -6.2113e-01,  2.2509e-01,  1.4516e+00,\n",
      "         4.7888e-02,  4.6627e-01, -5.1743e-01,  9.6956e-02, -8.0101e-01,\n",
      "         4.4144e-01, -7.9316e-01, -3.9682e-02, -4.0150e-02, -1.9094e-01,\n",
      "        -1.2023e+00, -6.8497e-01,  2.6934e-01, -8.6543e-01,  4.5773e-01,\n",
      "        -5.5520e-01, -1.4468e-01,  2.9817e-01,  3.0325e-01,  8.2879e-01,\n",
      "         8.2478e-01,  5.3457e-01,  2.9448e-01,  7.6926e-01, -5.9701e-01,\n",
      "        -7.9239e-01, -7.7570e-01,  8.0721e-01,  1.8503e-01, -2.9483e-01,\n",
      "        -4.9838e-01, -7.6633e-01, -9.0285e-01, -8.9473e-01,  8.9042e-02,\n",
      "        -6.0575e-01,  5.0103e-01,  1.2648e+00,  9.7228e-01, -1.1535e+00,\n",
      "         1.7292e-01, -3.6045e-01, -2.2033e-01, -1.6311e-01,  4.7144e-01,\n",
      "         2.9459e-01, -1.5951e-01, -1.2075e+00,  2.2713e-01,  6.9301e-01,\n",
      "        -6.8806e-01, -1.3081e-01,  3.0397e-01, -1.3610e+00, -3.4112e-01,\n",
      "         3.0122e-01, -5.9755e-01,  1.2578e+00,  1.4458e-02, -1.4086e+00,\n",
      "         4.8518e-01, -3.7766e-01, -4.9859e-02,  1.4838e-01, -1.8581e-01,\n",
      "        -1.2128e+00, -5.2805e-01, -1.0902e-01,  5.6495e-01,  2.5050e-01,\n",
      "        -5.1025e-01, -1.7269e-01,  2.7200e-02,  3.3995e-01,  2.3103e-01,\n",
      "        -1.0669e-01,  1.5304e+00, -5.2732e-01, -3.6566e-01,  2.8000e-01,\n",
      "         8.7116e-01,  5.3044e-02, -8.2915e-02, -2.1422e-01, -4.7691e-01,\n",
      "         1.0777e+00,  2.7665e-01,  1.1779e+00, -1.1158e+00, -6.8488e-01,\n",
      "         2.9032e-02, -2.9168e-01,  8.2465e-02, -8.0464e-01,  6.9826e-01,\n",
      "        -7.1208e-01,  6.0399e-01, -8.4892e-02, -7.6844e-01, -2.9124e-01,\n",
      "         2.7195e-01, -3.3244e-01, -3.1802e-01, -2.1012e-01,  8.7314e-01,\n",
      "        -8.4080e-01, -1.0870e-01,  7.8662e-02, -4.4726e-03, -6.0142e-01,\n",
      "         1.2135e+00, -2.8614e-01, -3.7744e-02, -4.7838e-01, -2.5514e-01,\n",
      "         2.3004e-01,  1.2594e-01,  3.3547e-01,  2.2156e-02, -3.3046e-01,\n",
      "         3.6715e-01, -6.4929e-01,  1.0181e+00, -1.8896e-01,  2.9627e-01,\n",
      "         2.1678e-01, -8.1707e-01,  1.8723e-01,  5.5194e-01, -9.3922e-02,\n",
      "        -1.4912e-02,  2.2270e-01,  1.7774e-01, -4.6771e-01,  1.3969e-01,\n",
      "         6.0949e-01,  6.6555e-01, -5.8852e-01, -2.0145e-02, -1.6271e-01,\n",
      "        -4.1293e-01,  5.1487e-01,  2.8536e-01,  5.4633e-01,  6.9837e-03,\n",
      "         6.3182e-01,  9.5775e-02,  4.6478e-01, -6.7348e-01, -1.1029e-01,\n",
      "         8.5977e-01,  6.0483e-01,  4.9791e-01,  2.1726e-01, -3.3156e-01,\n",
      "         3.4515e-01, -3.2863e-01,  6.6393e-01,  1.4789e+00,  1.0684e-01,\n",
      "        -6.9281e-01,  7.1584e-02, -3.5190e-01,  2.8377e-02,  3.2929e-01,\n",
      "        -3.5631e-01,  9.8174e-02, -6.8689e-01, -7.7502e-01,  9.7980e-01,\n",
      "         5.7950e-01,  7.8873e-01, -4.0435e-01, -2.3633e-01, -1.9721e-01,\n",
      "         1.0589e-01, -5.6518e-01, -4.9387e-01,  3.3275e-01, -3.8668e-01,\n",
      "         6.4068e-02,  1.0158e-01, -2.2519e-01,  3.0771e-02, -5.7992e-01,\n",
      "         9.1781e-01, -3.0329e-01, -2.9946e-01,  1.2332e-01,  7.1479e-02,\n",
      "        -8.2872e-01, -9.7629e-01, -5.1920e-01,  1.2599e-01, -4.1996e-01,\n",
      "         1.8942e-01,  9.5183e-01,  3.5105e-01,  5.2585e-01, -8.9365e-01,\n",
      "         1.6592e-01, -9.3765e-02,  3.2542e-01,  2.0903e-01,  1.0277e-01,\n",
      "        -1.6057e-01, -1.1592e+00,  6.8336e-01,  3.1015e-01, -1.8046e-01,\n",
      "        -1.6958e-01, -7.4439e-03, -3.2151e-01,  1.5343e-01, -2.1599e-01,\n",
      "        -2.8531e-01, -8.4948e-01,  1.8329e-01, -4.8473e-02, -2.6384e-01,\n",
      "         9.0712e-01,  1.4798e-01,  3.6069e-01, -1.5838e-01,  1.4816e-01,\n",
      "         4.8566e-01, -4.9864e-02,  6.9584e-01, -1.4756e-01,  1.9810e-01,\n",
      "         6.2263e-03,  1.9780e-01,  1.6869e-01, -2.8662e-01, -7.9161e-01,\n",
      "         1.0705e-01, -7.9927e-01, -1.1423e-01, -1.7310e-01, -4.7942e-02,\n",
      "        -5.9637e-01, -6.0145e-01, -3.5087e-01, -9.7470e-01,  2.9083e-01,\n",
      "        -1.6665e-01, -1.4716e-01, -1.3847e-01, -7.0433e-01, -1.1559e+00,\n",
      "        -9.5633e-01,  9.2003e-02, -8.5078e-01,  3.9509e-01, -1.2733e-01,\n",
      "        -2.1964e-01, -1.0848e-01, -1.1612e-01, -2.4879e-01,  6.1398e-01,\n",
      "        -5.0870e-01,  9.7069e-01,  3.1788e-02, -2.5057e-01, -5.4394e-01,\n",
      "         3.3202e-02,  3.0234e-01, -2.7526e-01,  3.2256e-01, -1.8667e-02,\n",
      "         9.3297e-02, -1.3904e-01, -1.2546e-01,  2.9172e-01,  2.1874e-01,\n",
      "         1.2943e+00,  3.7717e-01, -9.3464e-01, -5.0674e-02,  1.2819e+00,\n",
      "        -3.2078e-01,  1.1158e-03, -7.2790e-02,  6.8364e-01,  3.7037e-01,\n",
      "         4.9636e-03,  3.3192e-01,  3.0816e-01,  9.4116e-01,  8.9549e-02,\n",
      "         1.7840e-01,  8.9108e-02, -2.3567e-01,  4.4946e-01,  1.0488e+00,\n",
      "         3.5855e-01, -4.0979e-01, -1.1615e+00,  5.2817e-01, -1.5351e+00,\n",
      "        -6.7462e-01,  3.9955e-01,  8.8353e-01, -4.6093e-02, -4.1842e-01,\n",
      "         1.9170e-01, -2.4149e-01,  8.2037e-01,  3.3585e-01,  2.2469e-01,\n",
      "        -9.9932e-01, -1.4730e-01,  5.0317e-01,  1.6369e-01,  5.6951e-01,\n",
      "        -4.5111e-01,  4.1971e-01,  1.5350e+01,  5.4364e-01, -9.0226e-02,\n",
      "         5.0367e-01,  4.5224e-01,  3.6293e-01, -6.6691e-01, -1.3763e-01,\n",
      "        -1.0390e+00,  1.5355e-02,  1.4076e+00,  7.0067e-03,  1.1135e-02,\n",
      "         2.6773e-01,  7.5598e-02,  1.1384e-02, -5.3369e-01,  7.2428e-01,\n",
      "         5.0151e-01, -8.5842e-01,  3.4292e-01,  2.8022e-01, -5.0450e-01,\n",
      "         4.1095e-01,  4.7454e-01,  8.8547e-01,  8.8941e-01, -7.2621e-01,\n",
      "         5.7649e-01,  1.2236e-01,  1.0012e+00, -2.7260e-01,  6.3821e-01,\n",
      "         5.5339e-01, -1.0317e+00, -6.9508e-01, -3.9812e-01, -9.4133e-01,\n",
      "         2.1566e-01,  3.3889e-02, -4.8710e-01, -1.5931e-01, -3.7596e-01,\n",
      "         4.0801e-01,  7.9822e-01,  4.9310e-01,  6.8853e-02,  5.2982e-01,\n",
      "        -4.0713e-01, -3.5547e-01, -2.4605e-02,  5.8613e-01,  7.5624e-01,\n",
      "         1.1341e-01, -6.7450e-04,  1.6682e-01,  4.1556e-01,  1.4664e-01,\n",
      "        -3.1626e-01, -9.8911e-02, -6.0401e-01, -6.5963e-01, -2.6307e-01,\n",
      "         2.9054e-01,  5.5363e-01,  3.0632e-01, -4.5054e-01,  4.0632e-01,\n",
      "         2.8039e-01,  1.5155e-01, -3.5897e-01, -2.3669e-01,  2.9621e-01,\n",
      "        -1.0143e-01, -8.0246e-02,  1.1221e-01,  2.0057e-01, -2.9823e-01,\n",
      "        -1.0775e+00,  8.2978e-02,  7.4162e-01, -7.9720e-01, -7.3564e-01,\n",
      "         8.1612e-01,  2.2422e-01, -4.7371e-01,  2.5309e-01, -9.3964e-01,\n",
      "        -4.3246e-02,  1.4684e-01, -1.1383e+00, -5.2045e-01,  3.0995e-01,\n",
      "         6.8567e-03, -5.5267e-01,  3.8322e-01,  1.4942e+00,  6.9422e-02,\n",
      "        -3.5170e-01, -3.2438e-01,  4.9566e-01,  1.2208e-02, -3.6582e-01,\n",
      "        -9.1238e-01,  2.7467e-01,  9.4430e-02, -7.4476e-02,  3.7088e-01,\n",
      "         1.1164e-02,  3.4462e-02, -6.7783e-01, -4.2931e-01,  6.0321e-01,\n",
      "        -7.4381e-01, -3.0746e-01, -8.9220e-01, -4.5130e-01,  3.2326e-01,\n",
      "         2.3964e-01, -5.4592e-01,  7.0502e-01, -6.5838e-03, -4.1209e-01,\n",
      "        -2.1754e-01, -8.9195e-01,  2.0453e-01,  7.5011e-01, -5.3999e-01,\n",
      "        -6.6877e-01,  4.8704e-02,  1.1646e-01, -8.0286e-01, -6.6813e-02,\n",
      "        -2.1417e-01, -7.6149e-02,  6.7841e-02,  9.9334e-01, -4.1023e-01,\n",
      "         2.2446e-01,  6.3128e-01, -4.5110e-01, -6.5193e-01, -2.8113e-01,\n",
      "        -7.6455e-01, -1.3057e-01, -5.7492e-01,  5.9421e-01, -3.0563e-01,\n",
      "         2.2371e-01,  6.6584e-01,  3.4593e-01, -1.2022e-01, -8.5781e-01,\n",
      "        -5.5175e-01, -1.0249e-01, -5.2834e-01,  4.3773e-01, -4.5114e-01,\n",
      "         2.8637e-01, -1.4523e-01,  6.7809e-01,  4.3408e-01, -6.2642e-01,\n",
      "        -6.6808e-01,  4.9623e-01, -1.6559e-01, -1.7202e-01, -4.4601e-01,\n",
      "        -9.7212e-01, -1.5225e+00,  1.4690e-01, -1.0728e+00,  1.7742e-01,\n",
      "        -1.0633e+00, -2.9033e-01,  6.8458e-02, -4.1542e-01, -1.0342e-01,\n",
      "         7.6905e-01, -5.9405e-01, -3.0925e-01, -2.5278e-01, -3.6838e-01,\n",
      "         5.4287e-01,  4.7953e-01, -6.8533e-01,  3.9244e-01,  4.3543e-02,\n",
      "        -1.9725e-01,  3.2691e-01,  3.1394e-01, -3.6282e-01, -4.6558e-01,\n",
      "        -1.0793e+00,  1.6057e-01,  9.1008e-02, -1.9996e-01, -5.9777e-01,\n",
      "         2.3943e-01,  1.9583e-01, -1.7185e-01, -4.0545e-02,  8.9589e-01,\n",
      "        -1.0758e+00, -3.6750e-01,  6.1261e-01, -6.7153e-01,  1.3451e-01,\n",
      "         9.0921e-03, -9.9423e-02, -4.1234e-01,  3.9932e-01, -1.4934e-01,\n",
      "        -1.0674e+00, -5.2334e-01,  4.2787e-01, -1.1692e+00, -4.4434e-01,\n",
      "         3.5983e-01, -5.3816e-01, -6.5306e-01, -6.8869e-01, -1.9245e-01,\n",
      "         3.2994e-01, -4.0764e-01,  1.2078e+00,  4.8166e-01, -8.2703e-01,\n",
      "         1.7291e-01,  3.8458e-01,  2.4724e-01, -5.1415e-01, -2.1921e-02,\n",
      "        -2.2303e-02, -4.5586e-01,  5.8707e-01,  3.8949e-01,  3.4809e-01,\n",
      "        -1.2771e+00, -3.2686e-01,  5.5734e-01, -2.1396e-01, -2.9441e-01,\n",
      "         1.2112e+00, -2.5049e-01, -7.9551e-01,  3.2354e-01, -1.2211e+00,\n",
      "        -9.7717e-01, -5.6183e-01,  6.8158e-01,  8.1208e-02, -3.3837e-01,\n",
      "        -5.9036e-01, -5.0198e-01, -2.8759e-01, -1.2236e-01, -8.8443e-01,\n",
      "         2.2982e-01, -3.0118e-01, -6.2292e-01,  8.5303e-01,  7.5565e-01,\n",
      "        -4.8775e-01,  2.2856e-01, -6.2167e-02, -2.7615e-01, -6.8055e-01,\n",
      "         1.3529e-01, -4.3032e-01, -1.1112e-02,  4.3532e-01,  5.2108e-01,\n",
      "         3.5968e-01, -2.8047e-01, -2.2311e-01, -9.1597e-03,  6.3499e-01,\n",
      "         4.2089e-01, -4.6680e-01, -6.1733e-01,  1.4411e+00,  1.5937e+00,\n",
      "        -5.8288e-01, -1.6461e-01, -7.1794e-01, -5.5136e-01,  7.6791e-01,\n",
      "         3.3198e-01,  1.3084e-01,  1.1504e+00,  1.1937e-02,  1.6241e-01,\n",
      "         5.7800e-02, -7.3930e-01, -3.4276e-01,  3.4955e-01,  1.2565e+00,\n",
      "         1.0630e+00,  1.0626e+00, -5.0464e-01,  5.8582e-01, -5.8137e-03,\n",
      "         5.0452e-01,  9.4302e-01,  7.3980e-01, -3.6988e-02, -7.2750e-01,\n",
      "         2.5877e-01,  9.6205e-01, -5.3879e-01, -3.0058e-01, -8.2975e-02,\n",
      "         8.9210e-01, -4.9224e-02,  7.2432e-01,  6.4186e-01, -5.4799e-02,\n",
      "         4.6088e-01,  3.0663e-01,  4.3719e-01, -1.0991e+00, -7.9346e-02,\n",
      "        -3.7268e-01,  2.7887e-01,  8.4632e-02, -5.0961e-01, -3.4991e-01,\n",
      "        -6.4432e-01, -1.5452e-01,  1.5484e-01,  6.5290e-02,  3.0638e-01,\n",
      "         8.7094e-01,  6.5178e-01,  1.9598e-01, -4.1266e-01,  2.1996e-01,\n",
      "        -5.7109e-01, -7.4084e-01, -6.0625e-01, -8.9013e-01, -3.3879e-01,\n",
      "        -4.8722e-01,  1.4529e-01, -7.4197e-01])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "temp_encode = torch.randn(3, 768)\n",
    "save_path = '/usr/xtmp/hc387/ai_reviewer/LitSearch/eval/temp_testing/temp_pickle'\n",
    "temp_dict = {\n",
    "    \"index_name\": 'specter2',\n",
    "    \"index_type\":  'specter2',\n",
    "    \"keys\": [\"title1[SEP]abstract1\", \"title2[SEP]abstract2, title2[SEP]abstract2\"],\n",
    "    \"values\": [123, 455, 654],\n",
    "    \"encoded_keys\": temp_encode,\n",
    "    \"key_instruction\": \"Represent the title and abstract of the research paper for retrieval\",\n",
    "    \"query_instruction\": \"Represent the research question for retrieving relevant research paper abstracts\",\n",
    "    \"model_path\": 'allenai/specter2_base'\n",
    "}\n",
    "\n",
    "with open(save_path, 'wb') as file:\n",
    "    pickle.dump(temp_dict, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/hc387/miniconda3/envs/litsearch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertAdapterModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 40920.04it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecter2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SPECTER2\n\u001b[1;32m     11\u001b[0m index_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musr/xtmp/hc387/ai_reviewer/LitSearch/eval/temp_testing/temp_pickle\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 12\u001b[0m specter2_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mSPECTER2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m specter2_tensor \u001b[38;5;241m=\u001b[39m specter2_tensor\u001b[38;5;241m.\u001b[39mload(index_path)\n\u001b[1;32m     14\u001b[0m specter2_tensor\u001b[38;5;241m.\u001b[39msave_as_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/project/xtmp/hc387/ai_reviewer/LitSearch/eval/retrieval/specter2.py:21\u001b[0m, in \u001b[0;36mSPECTER2.__init__\u001b[0;34m(self, index_name, key_instruction, query_instruction, save_as_tensor, model_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m AutoAdapterModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mload_adapter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallenai/specter2\u001b[39m\u001b[38;5;124m\"\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf\u001b[39m\u001b[38;5;124m\"\u001b[39m, load_as\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecter2\u001b[39m\u001b[38;5;124m\"\u001b[39m, set_active\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_instruction \u001b[38;5;241m=\u001b[39m key_instruction\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_instruction \u001b[38;5;241m=\u001b[39m query_instruction\n",
      "File \u001b[0;32m~/miniconda3/envs/litsearch/lib/python3.8/site-packages/transformers/modeling_utils.py:2958\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2954\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2957\u001b[0m         )\n\u001b[0;32m-> 2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/litsearch/lib/python3.8/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/litsearch/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/litsearch/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/litsearch/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/litsearch/lib/python3.8/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/litsearch/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/litsearch/lib/python3.8/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "import argparse\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from utils import utils\n",
    "from eval.retrieval.kv_store import KVStore\n",
    "from eval.retrieval.specter2 import SPECTER2\n",
    "\n",
    "index_path = 'usr/xtmp/hc387/ai_reviewer/LitSearch/eval/temp_testing/temp_pickle'\n",
    "specter2_tensor = SPECTER2(None, None, None)\n",
    "specter2_tensor = specter2_tensor.load(index_path)\n",
    "specter2_tensor.save_as_tensor = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/hc387/miniconda3/envs/litsearch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "from utils import utils\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import ast\n",
    "from threading import Lock\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "class ManualClean():\n",
    "    def __init__(self, input_folder: str, output_dir: str, model_path: str = \"allenai/specter2_base\"):\n",
    "        # input output dir\n",
    "        self._input_folder = Path(input_folder)\n",
    "        self._output_dir = output_dir\n",
    "\n",
    "        #specter v2 model config and load\n",
    "        self.model_path = model_path\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self._model = AutoAdapterModel.from_pretrained(self.model_path)\n",
    "        self._model.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"specter2\", set_active=True)\n",
    "        \n",
    "        #index \n",
    "        self.index_name = 'specter2'\n",
    "        self.index_type = 'specter2'\n",
    "        self.key_instruction = \"Represent the title and abstract of the research paper for retrieval:\"\n",
    "        self.query_instruction = \"Represent the research question for retrieving relevant research paper abstracts:\"\n",
    "        self.keys = []\n",
    "        self.values = []\n",
    "        self.encoded_keys = []\n",
    "\n",
    "        #others\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "\n",
    "    def process_each_file(self, file_path: Path, lock) -> None:\n",
    "        #key is title and abstract\n",
    "        #value is corpusid\n",
    "        #embedding is torch.tensor\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for item in data:\n",
    "            value = utils.get_clean_corpusid(data[item])\n",
    "\n",
    "            #title and abstract\n",
    "            abstract = utils.get_clean_abstract(data[item])\n",
    "            try:\n",
    "                title = utils.get_clean_title(data[item])\n",
    "            except KeyError as e:\n",
    "                title = ''\n",
    "            key = f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "\n",
    "            #embeddings\n",
    "            try:\n",
    "                embedding = ast.literal_eval(data[item]['embedding_v2_vector'])\n",
    "                embedding = torch.tensor(embedding).requires_grad_(False)\n",
    "                if len(embedding) != 768:\n",
    "                    raise KeyError(f\"embedding length is not 768\")\n",
    "            except KeyError as e:\n",
    "                self._logger.error(f'No specter2 embedding for corpusid: {utils.get_clean_corpusid(data[item])}')\n",
    "\n",
    "                papers = [{'title': title, 'abstract': abstract}]\n",
    "                with lock:\n",
    "                    text_batch = [d['title'] + self._tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\n",
    "                    \n",
    "                    # preprocess the input\n",
    "                    inputs = self._tokenizer(text_batch, padding=True, truncation=True,\n",
    "                                                    return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
    "                    output = self._model(**inputs)\n",
    "\n",
    "                # take the first token in the batch as the embedding\n",
    "                embeddings = output.last_hidden_state[:, 0, :]\n",
    "                embedding =  embeddings[0].detach().clone().requires_grad_(False)\n",
    "            \n",
    "            with lock:\n",
    "                self.keys.append(key)\n",
    "                self.values.append(value)\n",
    "                self.encoded_keys.append(embedding)\n",
    "    \n",
    "    def process_all_files(self) -> None:\n",
    "\n",
    "        lock = Lock()\n",
    "        \n",
    "        json_files = list(self._input_folder.glob('*.json'))\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = {\n",
    "                    executor.submit(self.process_each_file, file_path, lock): file_path\n",
    "                    for file_path in json_files\n",
    "                }\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=len(json_files), desc='Processing all files'):\n",
    "                file_path = futures[future]\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as exc:\n",
    "                    self._logger.error(f'{file_path} generated an exception: {exc}')\n",
    "\n",
    "    def return_vals(self, save_as_tensor: bool = False) -> None:\n",
    "        if save_as_tensor:\n",
    "            self.encoded_keys = torch.stack(self.encoded_keys)\n",
    "        print(self.keys)\n",
    "        print(self.values)\n",
    "        print(self.encoded_keys)\n",
    "    \n",
    "    def save(self, save_as_tensor: bool = False) -> None:\n",
    "        savetype = 'list'\n",
    "        if save_as_tensor:\n",
    "            self.encoded_keys = torch.stack(self.encoded_keys)\n",
    "            savetype = 'tensor'\n",
    "        save_dict = {}\n",
    "        for key, value in self.__dict__.items():\n",
    "            if key[0] != \"_\":\n",
    "                save_dict[key] = value\n",
    "\n",
    "        print(f\"Saving index to {os.path.join(self._output_dir, f'{savetype}_{self.index_name}.{self.index_type}')}\")\n",
    "        os.makedirs(self._output_dir, exist_ok=True)\n",
    "        with open(os.path.join(self._output_dir, f\"{savetype}_{self.index_name}.{self.index_type}\"), 'wb') as file:\n",
    "            pickle.dump(save_dict, file, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertAdapterModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 25191.02it/s]\n",
      "ERROR:__main__:No specter2 embedding for corpusid: 1\n",
      "ERROR:__main__:No specter2 embedding for corpusid: 5\n",
      "Processing all files:   0%|          | 0/2 [00:00<?, ?it/s]ERROR:__main__:No specter2 embedding for corpusid: 2\n",
      "ERROR:__main__:No specter2 embedding for corpusid: 6\n",
      "ERROR:__main__:No specter2 embedding for corpusid: 3\n",
      "ERROR:__main__:No specter2 embedding for corpusid: 7\n",
      "ERROR:__main__:No specter2 embedding for corpusid: 4\n",
      "ERROR:__main__:No specter2 embedding for corpusid: 8\n",
      "Processing all files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving index to /usr/xtmp/hc387/ai_reviewer/LitSearch/eval/temp_testing/temp_folder/tensor_specter2.specter2\n"
     ]
    }
   ],
   "source": [
    "input_dir = '/usr/xtmp/hc387/ai_reviewer/LitSearch/eval/temp_testing/temp_folder'\n",
    "output_dir = '/usr/xtmp/hc387/ai_reviewer/LitSearch/eval/temp_testing/temp_folder'\n",
    "temp = ManualClean(input_dir, output_dir)\n",
    "temp.process_all_files()\n",
    "temp.save(save_as_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/usr/xtmp/hc387/ai_reviewer/LitSearch/eval/temp_testing/temp_folder/tensor_specter2.specter2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/usr/xtmp/hc387/ai_reviewer/LitSearch/eval/temp_testing/temp_folder/tensor_specter2.specter2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the pickle file\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Use the loaded data\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/litsearch/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/xtmp/hc387/ai_reviewer/LitSearch/eval/temp_testing/temp_folder/tensor_specter2.specter2'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to your pickle file\n",
    "file_path = '/usr/xtmp/hc387/ai_reviewer/LitSearch/eval/temp_testing/temp_folder/tensor_specter2.specter2'\n",
    "\n",
    "# Load the pickle file\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Use the loaded data\n",
    "print(data['keys'])\n",
    "print(data['values'])\n",
    "print(data['encoded_keys'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "768\n",
      "768\n",
      "768\n",
      "768\n",
      "768\n",
      "768\n",
      "768\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "import torch\n",
    "path_file = '/usr/xtmp/hc387/ai_reviewer/data/semantic_scholar/2024_10_8/everything_combined_more/file_621.json'\n",
    "\n",
    "with open(path_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for item in data:\n",
    "    embedding = ast.literal_eval(data[item]['embedding_v2_vector'])\n",
    "    embedding = torch.tensor(embedding).requires_grad_(False)\n",
    "\n",
    "    print(len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5656, -0.6054],\n",
      "        [-0.3655,  0.3068],\n",
      "        [ 0.7903, -0.1476],\n",
      "        [-0.2881, -0.4693],\n",
      "        [-1.4547,  0.5003],\n",
      "        [ 0.1156,  0.1400],\n",
      "        [-0.2620,  0.7061],\n",
      "        [ 2.6374, -0.1449],\n",
      "        [-0.3929,  1.8097],\n",
      "        [ 1.0943,  0.7799]])\n",
      "tensor([ 0.5656, -0.3655,  0.7903, -0.2881, -1.4547,  0.1156, -0.2620,  2.6374,\n",
      "        -0.3929,  1.0943])\n",
      "torch.return_types.topk(\n",
      "values=tensor([2.6374, 1.0943]),\n",
      "indices=tensor([7, 9]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "query = torch.tensor([1, 0]).float()\n",
    "\n",
    "encoded_keys = torch.randn(10, 2)\n",
    "print(encoded_keys)\n",
    "\n",
    "x = torch.matmul(encoded_keys, query)\n",
    "\n",
    "print(x)\n",
    "print(torch.topk(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litsearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
