{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import threading\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from threading import Lock\n",
    "from tqdm import tqdm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_line_by_line(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())  # Ensure to strip any whitespace including newline\n",
    "\n",
    "\n",
    "class JSONFilter:\n",
    "    def __init__(self, titles_file: str, input_directory: str):\n",
    "        self.titles_file = titles_file\n",
    "        self.input_dir = Path(input_directory)\n",
    "\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        self.titles_set = self.load_titles()\n",
    "\n",
    "        # Initialize a lock for thread-safe operations\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "        self.matched_records = {}\n",
    "\n",
    "    def load_titles(self):\n",
    "        \"\"\"Load titles from the text file into a set.\"\"\"\n",
    "        titles = set()\n",
    "        with open(self.titles_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                title = line.strip()\n",
    "                if title:\n",
    "                    titles.add(title)\n",
    "        return titles\n",
    "        \n",
    "    def process_file(self, json_file: Path) -> None:\n",
    "        \"\"\"Process a single JSON file, matching records based on the title.\"\"\"\n",
    "        try:\n",
    "            counts = 0\n",
    "            print(f'Processing {json_file.name} now')\n",
    "            local_matched_records = []\n",
    "            for data in load_file_line_by_line(json_file):\n",
    "                if 'title' in data:\n",
    "                    # Process the title from the JSON record\n",
    "                    processed_title = data['title'].lower()\n",
    "                    if processed_title in self.titles_set:\n",
    "                        # Extract corpusid and original title\n",
    "                        with self.lock:\n",
    "                            self.matched_records[data['corpusid']] = {\n",
    "                                \"corpusid\": data.get('corpusid'),\n",
    "                                \"title\": data.get('title')\n",
    "                            }\n",
    "                        counts += 1\n",
    "                else:\n",
    "                    self.logger.warning(f\"No 'title' in record in file {json_file.name}\")\n",
    "\n",
    "            print(f'For {json_file.name}, found {counts} matching records')\n",
    "            self.logger.info(f\"Processed and matched: {json_file.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {json_file.name}: {str(e)}\")\n",
    "\n",
    "    def process_all_files(self, max_workers: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Process all JSON files in parallel using a thread pool.\n",
    "\n",
    "        Args:\n",
    "            max_workers: Maximum number of threads to use\n",
    "        \"\"\"\n",
    "        json_files = list(self.input_dir.glob('*.json'))\n",
    "\n",
    "        if not json_files:\n",
    "            self.logger.warning(f\"No JSON files found in {self.input_dir}\")\n",
    "            return\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            executor.map(self.process_file, json_files)\n",
    "\n",
    "    def return_found(self):\n",
    "        return self.matched_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class combineFiles:\n",
    "    def __init__(self, categories: dict, main_dict: dict, output_dir: str, max_workers = 10):\n",
    "        self.categories = categories\n",
    "        self.main_dict = main_dict\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.output_dir = output_dir\n",
    "        self.max_workers = max_workers\n",
    "\n",
    "    def process_file(self, file_path, lock):\n",
    "        count = 0 \n",
    "        self.logger.info(f\"Processed file: {file_path}\")\n",
    "        for item in load_file_line_by_line(file_path):\n",
    "            corpusid = item['corpusid']\n",
    "\n",
    "            if corpusid in self.main_dict:\n",
    "                count+=1\n",
    "\n",
    "                if re.search('embeddings_v1', str(file_path), re.IGNORECASE):\n",
    "                    key_mapping = {\"model\": \"embedding_v1\", \"vector\": \"embedding_v1_vector\"}\n",
    "                    for old_key, new_key in key_mapping.items():\n",
    "                        if old_key in item:\n",
    "                            item[new_key] = item.pop(old_key)\n",
    "\n",
    "                if re.search('embeddings_v2', str(file_path), re.IGNORECASE):\n",
    "                    key_mapping = {\"model\": \"embedding_v2\", \"vector\": \"embedding_v2_vector\"}\n",
    "                    for old_key, new_key in key_mapping.items():\n",
    "                        if old_key in item:\n",
    "                            item[new_key] = item.pop(old_key)\n",
    "                \n",
    "                with lock:\n",
    "                    self.main_dict[corpusid].update(item)\n",
    "        self.logger.info(f\"Finished processing: {file_path.name}. Found {count} matching records\")\n",
    "\n",
    "    def process_category(self):\n",
    "        \"\"\"Process all JSON files within all category folders concurrently.\"\"\"\n",
    "        lock = Lock()\n",
    "        all_files = []\n",
    "\n",
    "        # Collect all JSON files from all categories\n",
    "        for category in self.categories:\n",
    "            json_files = list(Path(self.categories[category]).glob('*.json'))\n",
    "            all_files.extend(json_files)\n",
    "\n",
    "        total_files = len(all_files)\n",
    "        self.logger.info(f\"Total files to process: {total_files}\")\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Submit all files to the executor\n",
    "            futures = {\n",
    "                executor.submit(self.process_file, file_path, lock): file_path\n",
    "                for file_path in all_files\n",
    "            }\n",
    "\n",
    "            # Use tqdm to track the progress of all futures\n",
    "            for future in tqdm(as_completed(futures), total=total_files, desc='Processing all files'):\n",
    "                file_path = futures[future]\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as exc:\n",
    "                    self.logger.error(f'{file_path} generated an exception: {exc}')\n",
    "\n",
    "    def write_main_dict_to_file(self):\n",
    "        with open(self.output_dir, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.main_dict, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processed and matched: paper1.json\n",
      "INFO:__main__:Processed and matched: paper2.json\n",
      "INFO:__main__:Total files to process: 4\n",
      "INFO:__main__:Processed file: /usr/xtmp/hc387/ai_reviewer/data/iclr/temp_embeddings_v1/file2.json\n",
      "INFO:__main__:Processed file: /usr/xtmp/hc387/ai_reviewer/data/iclr/temp_embeddings_v1/file1.json\n",
      "INFO:__main__:Processed file: /usr/xtmp/hc387/ai_reviewer/data/iclr/temp_embeddings_v2/file2.json\n",
      "INFO:__main__:Processed file: /usr/xtmp/hc387/ai_reviewer/data/iclr/temp_embeddings_v2/file1.json\n",
      "INFO:__main__:Finished processing: file2.json. Found 3 matching records\n",
      "INFO:__main__:Finished processing: file1.json. Found 2 matching records\n",
      "INFO:__main__:Finished processing: file1.json. Found 2 matching records\n",
      "INFO:__main__:Finished processing: file2.json. Found 3 matching records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing paper1.json now\n",
      "Processing paper2.json now\n",
      "For paper1.json, found 2 matching records\n",
      "For paper2.json, found 3 matching records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all files: 100%|██████████| 4/4 [00:00<00:00, 9393.74it/s]\n"
     ]
    }
   ],
   "source": [
    "title_file = '/usr/xtmp/hc387/ai_reviewer/data/iclr/temp_titles.txt'\n",
    "input_folder = '/usr/xtmp/hc387/ai_reviewer/data/iclr/temp_papers'\n",
    "output_dir = '/usr/xtmp/hc387/ai_reviewer/data/iclr/temp_results.json'\n",
    "\n",
    "categories = {\n",
    "    \"temp1\": '/usr/xtmp/hc387/ai_reviewer/data/iclr/temp_embeddings_v1',\n",
    "    \"temp2\": '/usr/xtmp/hc387/ai_reviewer/data/iclr/temp_embeddings_v2'\n",
    "}\n",
    "\n",
    "jsonfilter = JSONFilter(title_file, input_folder)\n",
    "jsonfilter.process_all_files(2)\n",
    "found = jsonfilter.return_found()\n",
    "\n",
    "combiner = combineFiles(categories, found, output_dir)\n",
    "combiner.process_category()\n",
    "combiner.write_main_dict_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
