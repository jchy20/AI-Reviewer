{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import gzip\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'x-api-key': 'PB2IUQCkgU7VqjOeJ1v7W3h688WDRxrnaialTY1q',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "r1 = requests.get('https://api.semanticscholar.org/datasets/v1/release/2024-10-08', headers = headers).json()\n",
    "print(json.dumps(r1, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_citations.json\n"
     ]
    }
   ],
   "source": [
    "# one file check\n",
    "\n",
    "file_url = files[0]\n",
    "dataset = '2024-10-8'\n",
    "file_name = 'temp_citations.json'\n",
    "print(file_name)\n",
    "\n",
    "with requests.get(file_url, headers=headers, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    # Decompress on-the-fly and write to output file\n",
    "    with gzip.open(r.raw, 'rb') as decompressed_stream:\n",
    "        with open(file_name, 'wb') as out_file:\n",
    "            shutil.copyfileobj(decompressed_stream, out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entire file check\n",
    "\n",
    "dataset = '2024-10-8'\n",
    "\n",
    "# Rate limiting parameters\n",
    "RATE_LIMIT = 1  # Max requests per second\n",
    "MIN_REQUEST_INTERVAL = 1.0 / RATE_LIMIT  # Minimum interval between requests in seconds\n",
    "last_request_time = 0  # Time of the last request\n",
    "\n",
    "for idx, file_url in enumerate(files):\n",
    "    file_name = f'{dataset}_file{idx}.json'\n",
    "    print(f\"Downloading file {idx + 1}/{len(files)}: {file_url}\")\n",
    "\n",
    "    success = False\n",
    "    retries = 0\n",
    "    max_retries = 5  # Maximum number of retries for each file\n",
    "    delay = 1  # Initial delay for exponential backoff\n",
    "\n",
    "    while not success and retries < max_retries:\n",
    "        # Rate limiting: Ensure at least 1 second between requests\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_request_time\n",
    "        if elapsed < MIN_REQUEST_INTERVAL:\n",
    "            sleep_time = MIN_REQUEST_INTERVAL - elapsed\n",
    "            print(f\"Rate limit enforced. Sleeping for {sleep_time:.2f} seconds.\")\n",
    "            time.sleep(sleep_time)\n",
    "        last_request_time = time.time()  # Update last request time\n",
    "\n",
    "        try:\n",
    "            with requests.get(file_url, headers=headers, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                with gzip.open(r.raw, 'rb') as decompressed_stream:\n",
    "                    with open(file_name, 'wb') as out_file:\n",
    "                        shutil.copyfileobj(decompressed_stream, out_file)\n",
    "            success = True  # Download succeeded\n",
    "            print(f\"Successfully downloaded {file_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {file_url}: {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff: double the delay\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this file.\")\n",
    "    if not success:\n",
    "        print(f\"Failed to download {file_url} after {max_retries} attempts.\")\n",
    "        continue  # Skip to the next file if there's an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "keywords = ['nlp', 'large language model', 'language model', 'llm', 'large language models', 'language models', 'llms']\n",
    "file_path = '2024_10_8/abstracts/abstracts_file0.json'\n",
    "\n",
    "def load_file_line_by_line(file_path):\n",
    "    '''\n",
    "    return an iterable file\n",
    "    '''\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)  # Converts each line from JSON string to a Python dictionary\n",
    "\n",
    "def return_matching_dict(file_path, corpusid):\n",
    "    \"\"\"\n",
    "    return a dict of matching corpusid\n",
    "    \"\"\"\n",
    "    for item in load_file_line_by_line(file_path):\n",
    "        if item['corpusid'] == corpusid:\n",
    "            return item\n",
    "    return None\n",
    "\n",
    "def check_abstract(abstract, keywords):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    abstract: text of abstract\n",
    "    keywords: a list of keywords\n",
    "    return whether a abstract contains at least one keyword\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(keyword) for keyword in keywords) + r')\\b'\n",
    "    return bool(re.search(pattern, abstract, re.IGNORECASE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = 'temp_embedding_v1.json'  # Replace with your file path\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line_number, line in enumerate(file, start=1):\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            # Print the entire JSON object\n",
    "            print(f\"Line {line_number}: {json.dumps(data, indent=2)}\\n\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON on line {line_number}: {e}\")\n",
    "            continue  # Skip lines that can't be parsed\n",
    "        # Optional: Stop after a certain number of lines\n",
    "        if line_number >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os  # Added to handle file paths\n",
    "import requests\n",
    "import shutil\n",
    "import gzip\n",
    "import itertools\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Constants\n",
    "dataset = '2024-10-8'\n",
    "headers = {\n",
    "    'x-api-key': 'PB2IUQCkgU7VqjOeJ1v7W3h688WDRxrnaialTY1q',\n",
    "    'Accept': 'application/json'\n",
    "}  # Replace with your API key\n",
    "RATE_LIMIT = 1  # Max requests per second\n",
    "MIN_REQUEST_INTERVAL = 1.0 / RATE_LIMIT  # Minimum interval between requests in seconds\n",
    "\n",
    "# Thread-safe counter for file indexing\n",
    "index_counter = itertools.count()\n",
    "\n",
    "# File download function\n",
    "def download_file(file_url, dataset, headers, folder_path):\n",
    "    # Use the next value of the atomic counter as the index for this file\n",
    "    idx = next(index_counter)\n",
    "    file_name = f'{dataset}_file{idx}.json'\n",
    "    \n",
    "    # Create the full file path in the specified folder\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    print(f\"Downloading file {idx + 1}: {file_url}\")\n",
    "\n",
    "    success = False\n",
    "    retries = 0\n",
    "    max_retries = 5  # Maximum number of retries for each file\n",
    "    delay = 1  # Initial delay for exponential backoff\n",
    "    last_request_time = 0  # Time of the last request\n",
    "\n",
    "    while not success and retries < max_retries:\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_request_time\n",
    "        if elapsed < MIN_REQUEST_INTERVAL:\n",
    "            sleep_time = MIN_REQUEST_INTERVAL - elapsed\n",
    "            print(f\"Rate limit enforced. Sleeping for {sleep_time:.2f} seconds.\")\n",
    "            time.sleep(sleep_time)\n",
    "        last_request_time = time.time()  # Update last request time\n",
    "\n",
    "        try:\n",
    "            with requests.get(file_url, headers=headers, stream=True) as r:\n",
    "                r.raise_for_status()  # Raise an exception if the download failed\n",
    "                with gzip.open(r.raw, 'rb') as decompressed_stream:\n",
    "                    with open(file_path, 'wb') as out_file:  # Save to file_path\n",
    "                        shutil.copyfileobj(decompressed_stream, out_file)\n",
    "            success = True  # Download succeeded\n",
    "            print(f\"Successfully downloaded {file_path}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {file_url}: {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff: double the delay\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this file.\")\n",
    "    return success\n",
    "\n",
    "# Download files with multi-threading\n",
    "def download_all_files_concurrently(files, dataset, headers, folder_path):\n",
    "    max_workers = 5  # Adjust based on your system's capabilities\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(download_file, file_url, dataset, headers, folder_path): file_url for file_url in files}\n",
    "        \n",
    "        for future in as_completed(future_to_file):\n",
    "            file_url = future_to_file[future]\n",
    "            try:\n",
    "                success = future.result()\n",
    "                if not success:\n",
    "                    print(f\"Failed to download: {file_url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with future {file_url}: {e}\")\n",
    "\n",
    "def download_multiple_categories(categories, headers, base_folder):\n",
    "    with ThreadPoolExecutor(max_workers=len(categories)) as executor:  # One thread per category\n",
    "        future_to_category = {\n",
    "            executor.submit(download_category, category_name, urls, headers, base_folder): category_name \n",
    "            for category_name, urls in categories.items()\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_category):\n",
    "            category = future_to_category[future]\n",
    "            try:\n",
    "                future.result()  # Wait for the category download to complete\n",
    "                print(f\"Category '{category}' downloaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading category '{category}': {e}\")\n",
    "\n",
    "# Function to download all files in a category\n",
    "def download_category(category, urls, headers, base_folder):\n",
    "    dataset = category  # Category can be used as dataset name\n",
    "    folder_path = os.path.join(base_folder, category)  # Create folder for the category\n",
    "\n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    print(f\"Downloading category: {category} with {len(urls)} files.\")\n",
    "    download_all_files_concurrently(urls, dataset, headers, folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched data from https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/abstracts\n",
      "Successfully fetched data from https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/authors\n",
      "Successfully fetched data from https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/citations\n",
      "Successfully fetched data from https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/embeddings-specter_v1\n",
      "Successfully fetched data from https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/embeddings-specter_v2\n",
      "Successfully fetched data from https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/paper-ids\n",
      "Successfully fetched data from https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/papers\n",
      "Successfully fetched data from https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/publication-venues\n",
      "Successfully fetched data from https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/s2orc\n",
      "Successfully fetched data from https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/tldrs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "# Constants\n",
    "headers = {\n",
    "    'x-api-key': 'PB2IUQCkgU7VqjOeJ1v7W3h688WDRxrnaialTY1q',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# List of dataset endpoints\n",
    "endpoints = [\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/abstracts',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/authors',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/citations',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/embeddings-specter_v1',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/embeddings-specter_v2',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/paper-ids',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/papers',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/publication-venues',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/s2orc',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/tldrs'\n",
    "]\n",
    "\n",
    "# Dictionary to store the results for each dataset\n",
    "categories = {}\n",
    "category_names = ['abstracts', 'authors', 'citations', 'embeddings_v1', 'embeddings_v2', 'paper_ids', 'papers', 'publication_venues', 's2orc', 'tldrs']\n",
    "\n",
    "# Loop through each endpoint, making requests with a 2-second delay\n",
    "for idx, endpoint in enumerate(endpoints):\n",
    "    try:\n",
    "        response = requests.get(endpoint, headers=headers).json()\n",
    "        categories[category_names[idx]] = response.get('files', [])\n",
    "        print(f\"Successfully fetched data from {endpoint}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from {endpoint}: {e}\")\n",
    "    \n",
    "    # Wait for 2 seconds before the next request, except for the last iteration\n",
    "    if idx < len(endpoints) - 1:\n",
    "        time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstracts: 60\n",
      "authors: 30\n",
      "citations: 215\n",
      "embeddings_v1: 941\n",
      "embeddings_v2: 913\n",
      "paper_ids: 30\n",
      "papers: 60\n",
      "publication_venues: 1\n",
      "s2orc: 277\n",
      "tldrs: 30\n"
     ]
    }
   ],
   "source": [
    "for category_name, url in categories.items():\n",
    "    print(f'{category_name}: {len(url)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path1 = '2024_10_8'\n",
    "for category, url in categories.items():\n",
    "    folder_path = os.path.join(folder_path1, category)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os  # Added to handle file paths\n",
    "import requests\n",
    "import shutil\n",
    "import gzip\n",
    "import itertools\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "# Constants\n",
    "headers = {\n",
    "    'x-api-key': 'PB2IUQCkgU7VqjOeJ1v7W3h688WDRxrnaialTY1q',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# List of dataset endpoints\n",
    "endpoints = [\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/abstracts',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/authors',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/citations',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/embeddings-specter_v1',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/embeddings-specter_v2',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/paper-ids',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/papers',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/publication-venues',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/s2orc',\n",
    "    'https://api.semanticscholar.org/datasets/v1/release/2024-10-08/dataset/tldrs'\n",
    "]\n",
    "\n",
    "# Dictionary to store the results for each dataset\n",
    "categories = {}\n",
    "category_names = ['abstracts', 'authors', 'citations', 'embeddings_v1', 'embeddings_v2', 'paper_ids', 'papers', 'publication_venues', 's2orc', 'tldrs']\n",
    "\n",
    "# Loop through each endpoint, making requests with a 2-second delay\n",
    "for idx, endpoint in enumerate(endpoints):\n",
    "    try:\n",
    "        response = requests.get(endpoint, headers=headers).json()\n",
    "        categories[category_names[idx]] = response.get('files', [])\n",
    "        print(f\"Successfully fetched data from {endpoint}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from {endpoint}: {e}\")\n",
    "    \n",
    "    # Wait for 2 seconds before the next request, except for the last iteration\n",
    "    if idx < len(endpoints) - 1:\n",
    "        time.sleep(2)\n",
    "\n",
    "for category_name, url in categories.items():\n",
    "    print(f'{category_name}: {len(url)}')\n",
    "\n",
    "RATE_LIMIT = 1  # Max requests per second\n",
    "MIN_REQUEST_INTERVAL = 1.0 / RATE_LIMIT  # Minimum interval between requests in seconds\n",
    "\n",
    "\n",
    "# File download function\n",
    "def download_file(file_url, dataset, headers, folder_path, index_counter):\n",
    "    # Use the next value of the atomic counter as the index for this file\n",
    "    idx = next(index_counter)\n",
    "    file_name = f'{dataset}_file{idx}.json'\n",
    "    \n",
    "    # Create the full file path in the specified folder\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    print(f\"Downloading file {idx + 1}: {file_url}\")\n",
    "\n",
    "    success = False\n",
    "    retries = 0\n",
    "    max_retries = 5  # Maximum number of retries for each file\n",
    "    delay = 1  # Initial delay for exponential backoff\n",
    "    last_request_time = 0  # Time of the last request\n",
    "\n",
    "    while not success and retries < max_retries:\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_request_time\n",
    "        if elapsed < MIN_REQUEST_INTERVAL:\n",
    "            sleep_time = MIN_REQUEST_INTERVAL - elapsed\n",
    "            print(f\"Rate limit enforced. Sleeping for {sleep_time:.2f} seconds.\")\n",
    "            time.sleep(sleep_time)\n",
    "        last_request_time = time.time()  # Update last request time\n",
    "\n",
    "        try:\n",
    "            with requests.get(file_url, headers=headers, stream=True) as r:\n",
    "                r.raise_for_status()  # Raise an exception if the download failed\n",
    "                with gzip.open(r.raw, 'rb') as decompressed_stream:\n",
    "                    with open(file_path, 'wb') as out_file:  # Save to file_path\n",
    "                        shutil.copyfileobj(decompressed_stream, out_file)\n",
    "            success = True  # Download succeeded\n",
    "            print(f\"Successfully downloaded {file_path}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {file_url}: {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff: double the delay\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this file.\")\n",
    "    return success\n",
    "\n",
    "# Download files with multi-threading\n",
    "def download_all_files_concurrently(files, dataset, headers, folder_path, index_counter):\n",
    "    max_workers = 50  # Adjust based on your system's capabilities\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(download_file, file_url, dataset, headers, folder_path, index_counter): file_url for file_url in files}\n",
    "        \n",
    "        for future in as_completed(future_to_file):\n",
    "            file_url = future_to_file[future]\n",
    "            try:\n",
    "                success = future.result()\n",
    "                if not success:\n",
    "                    print(f\"Failed to download: {file_url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with future {file_url}: {e}\")\n",
    "\n",
    "\n",
    "folder_path1 = '2024_10_8'\n",
    "for category, url in categories.items():\n",
    "    folder_path = os.path.join(folder_path1, category)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # Thread-safe counter for file indexing\n",
    "    index_counter = itertools.count()\n",
    "    download_all_files_concurrently(url, category, headers, folder_path, index_counter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
