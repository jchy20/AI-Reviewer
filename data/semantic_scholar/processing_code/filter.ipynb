{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import threading\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_line_by_line(file_path):\n",
    "    '''\n",
    "    return an iterable file\n",
    "    '''\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)  # Converts each line from JSON string to a Python dictionary\n",
    "\n",
    "def check_abstract(abstract, keywords):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    abstract: text of abstract\n",
    "    keywords: a list of keywords\n",
    "    return whether a abstract contains at least one keyword\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(keyword) for keyword in keywords) + r')\\b'\n",
    "    return bool(re.search(pattern, abstract, re.IGNORECASE))\n",
    "\n",
    "def combine_dict(abstract_dict, dicts, category_names):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    abstract: text of abstract\n",
    "    dicts: a dict of dicts, keys are category names and values are the retreived corresponding parts of the paper\n",
    "    Return one combined dict\n",
    "    \"\"\"\n",
    "    temp_dict = abstract_dict.copy()\n",
    "    for category in category_names:\n",
    "        if category in dicts and dicts[category]:\n",
    "            temp_dict.update(dicts[category])\n",
    "\n",
    "    return temp_dict\n",
    "\n",
    "def fetch_from_file_with_stop(file_path, corpusid, result_queue, stop_event):\n",
    "    \"\"\"\n",
    "    Try to find matching record from one single json file\n",
    "    \"\"\"\n",
    "    print(f'processcing file {file_path}')\n",
    "    for item in load_file_line_by_line(file_path):\n",
    "        if stop_event.is_set():\n",
    "            return  # Another thread has already found the result, stop searching\n",
    "\n",
    "        if 'corpusid' in item and item['corpusid'] == corpusid:\n",
    "            result_queue.put(item)  # Put the result in the queue\n",
    "            stop_event.set()  # Signal other threads to stop searching\n",
    "            return\n",
    "\n",
    "\n",
    "def write_results(output_queue, output_file):\n",
    "    \"\"\"\n",
    "    Writes the combined records from the output queue to the final output file.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        while True:\n",
    "            record = output_queue.get()\n",
    "            if record is None:\n",
    "                break\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "def process_category(category_name, category_path, corpusid, num_workers):\n",
    "    \"\"\"\n",
    "    Process a single category using num_workers threads.\n",
    "    Each worker processes a different file in the category.\n",
    "    Use a ThreadPoolExecutor to manage worker threads.\n",
    "    \"\"\"\n",
    "    result_queue = Queue()\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    # Get the list of files in the category\n",
    "    files = [os.path.join(category_path, file) for file in os.listdir(category_path) if os.path.isfile(os.path.join(category_path, file))]\n",
    "\n",
    "    # Use a ThreadPoolExecutor to limit the number of concurrent threads\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = []\n",
    "        for file_path in files:\n",
    "            if stop_event.is_set():\n",
    "                break  # If a result is already found, stop submitting new tasks\n",
    "\n",
    "            futures.append(executor.submit(fetch_from_file_with_stop, file_path, corpusid, result_queue, stop_event))\n",
    "\n",
    "        # Wait for the results\n",
    "        for future in as_completed(futures):\n",
    "            if stop_event.is_set():\n",
    "                break  # If one of the threads found the result, stop waiting for others\n",
    "\n",
    "    # Return the result (if found)\n",
    "    if not result_queue.empty():\n",
    "        return result_queue.get()  # Return the first result found\n",
    "    else:\n",
    "        return None  # No result found\n",
    "\n",
    "\n",
    "def process_file(file_path, output_queue, categories, keywords, category_names, num_workers):\n",
    "    \"\"\"\n",
    "    Processes each file in the abstracts folder, checks the abstract for keywords, \n",
    "    fetches corresponding data from categories one by one, and combines the dictionaries.\n",
    "    \"\"\"\n",
    "    for abstract_record in load_file_line_by_line(file_path):\n",
    "        corpusid = abstract_record.get('corpusid')\n",
    "\n",
    "        # Check if the abstract contains any of the keywords\n",
    "        if 'abstract' in abstract_record and check_abstract(abstract_record['abstract'], keywords):\n",
    "            print('it did run')\n",
    "            # Dictionary to store the fetched components from different categories\n",
    "            fetched_data = {}\n",
    "\n",
    "            # Process each category one by one\n",
    "            for category in category_names:\n",
    "                if category == 'abstracts':\n",
    "                    continue\n",
    "                category_path = categories[category]\n",
    "                result = process_category(category, category_path, corpusid, num_workers)\n",
    "\n",
    "                if result:\n",
    "                    fetched_data[category] = result\n",
    "\n",
    "            # Combine the abstract dict with data from other categories\n",
    "            combined_record = combine_dict(abstract_record, fetched_data, category_names)\n",
    "            # Push the combined record to the output queue for writing\n",
    "            output_queue.put(combined_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = '2024_10_8'\n",
    "abstracts_folder = os.path.join(base_folder, 'abstracts')\n",
    "\n",
    "keywords = ['nlp', 'large language model', 'language model', 'llm', 'large language models', 'language models', 'llms']\n",
    "category_names = ['abstracts', 'embeddings_v1', 'embeddings_v2', 'paper_ids', 'papers', 's2orc', 'tldrs']\n",
    "\n",
    "# category_names = ['abstracts', 'authors', 'citations', 'embeddings_v1', 'embeddings_v2', 'paper_ids', 'papers', 'publication_venues', 's2orc', 'tldrs']\n",
    "\n",
    "categories = {}\n",
    "\n",
    "for category in category_names: \n",
    "    if category == 'abstracts':\n",
    "        continue\n",
    "    categories[category] = os.path.join(base_folder, category)\n",
    "\n",
    "\n",
    "num_workers = 1\n",
    "\n",
    "total = 0\n",
    "\n",
    "for idx, dir in enumerate(os.listdir(abstracts_folder)):\n",
    "\n",
    "    output_queue = Queue()\n",
    "\n",
    "    file_path = os.path.join(abstracts_folder, dir)\n",
    "\n",
    "    print(f'Start processing {file_path}')\n",
    "    process_file(file_path, output_queue, categories, keywords, category_names, num_workers)\n",
    "\n",
    "    output_name = f'combined_file_{idx}.json'\n",
    "    folder = os.path.join(base_folder, 'combined')\n",
    "    output_file_path = os.path.join(folder, output_name)\n",
    "    \n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        while not output_queue.empty():\n",
    "            combined_record = output_queue.get()\n",
    "            output_file.write(json.dumps(combined_record) + '\\n')\n",
    "            total += 1\n",
    "\n",
    "print(f'total paper: {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = ['nlp', 'large language model', 'language model', 'llm', 'large language models', 'language models', 'llms']\n",
    "text = 'model, today language'\n",
    "\n",
    "check_abstract(text, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entries(json_file_path):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)  # Load the entire JSON file into a Python object\n",
    "    return len(data)  \n",
    "\n",
    "input_dir = '2024_10_8/abstract_filtered'\n",
    "output_dir = '2024_10_8/abstract_filtered/all.json'\n",
    "\n",
    "combined_record = []\n",
    "for dir in os.listdir(input_dir):\n",
    "    input_file = os.path.join(input_dir, dir)\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "        if isinstance(data, list):\n",
    "            combined_record.extend(data)\n",
    "        else:\n",
    "            combined_record.append(data)\n",
    "    \n",
    "with open(output_dir, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_record, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150364\n"
     ]
    }
   ],
   "source": [
    "print(count_entries('2024_10_8/abstract_filtered/all.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
