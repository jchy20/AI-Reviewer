{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import threading\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from util.utils import load_file_line_by_line\n",
    "import logging\n",
    "from threading import Lock\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m----> 9\u001b[0m temp_dict \u001b[38;5;241m=\u001b[39m {item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpusid\u001b[39m\u001b[38;5;124m'\u001b[39m]: item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data}\n\u001b[1;32m     10\u001b[0m all_abstracts\u001b[38;5;241m.\u001b[39mupdate(temp_dict)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filterd_abstracts = '/usr/xtmp/hc387/ai_reviewer/data/semantic_scholar/2024_10_8/abstract_filtered_more'\n",
    "\n",
    "all_abstracts = {}\n",
    "\n",
    "for dir in os.listdir(filterd_abstracts):\n",
    "    file_path = os.path.join(filterd_abstracts, dir)\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    temp_dict = {item['corpusid']: item for item in data}\n",
    "    all_abstracts.update(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class combineFiles:\n",
    "    def __init__(self, categories: dict, main_dict: dict):\n",
    "        self.categories = categories\n",
    "        self.main_dict = main_dict\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def process_file(self, file_path, lock):\n",
    "        self.logger.info(f\"Processed file: {file_path}\")\n",
    "        for item in load_file_line_by_line(file_path):\n",
    "            corpusid = item['corpusid']\n",
    "            if corpusid in self.main_dict:\n",
    "                with lock:\n",
    "                    self.main_dict[corpusid].update(item)\n",
    "        self.logger.info(f\"Finished processing: {file_path}\")\n",
    "\n",
    "    def process_category(self, max_workers):\n",
    "        \"\"\"Process all JSON files within all category folders concurrently.\"\"\"\n",
    "        lock = Lock()\n",
    "        all_files = []\n",
    "\n",
    "        # Collect all JSON files from all categories\n",
    "        for category in self.categories:\n",
    "            json_files = list(Path(category).glob('*.json'))\n",
    "            all_files.extend(json_files)\n",
    "\n",
    "        total_files = len(all_files)\n",
    "        self.logger.info(f\"Total files to process: {total_files}\")\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all files to the executor\n",
    "            futures = {\n",
    "                executor.submit(self.process_file, file_path, lock): file_path\n",
    "                for file_path in all_files\n",
    "            }\n",
    "\n",
    "            # Use tqdm to track the progress of all futures\n",
    "            for future in tqdm(as_completed(futures), total=total_files, desc='Processing all files'):\n",
    "                file_path = futures[future]\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as exc:\n",
    "                    self.logger.error(f'{file_path} generated an exception: {exc}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['abstracts', 'embeddings_v1', 'embeddings_v2', 'paper_ids', 'papers', 's2orc', 'tldrs']\n",
    "base_folder = '/usr/xtmp/hc387/ai_reviewer/data/semantic_scholar/2024_10_8'\n",
    "# category_names = ['abstracts', 'authors', 'citations', 'embeddings_v1', 'embeddings_v2', 'paper_ids', 'papers', 'publication_venues', 's2orc', 'tldrs']\n",
    "\n",
    "categories = {}\n",
    "\n",
    "for category in category_names: \n",
    "    if category == 'abstracts':\n",
    "        continue\n",
    "    categories[category] = os.path.join(base_folder, category)\n",
    "\n",
    "# filterd_abstracts = '/usr/xtmp/hc387/ai_reviewer/data/semantic_scholar/2024_10_8/abstract_filtered_more'\n",
    "# all_abstracts = {}\n",
    "\n",
    "# for dir in os.listdir(filterd_abstracts):\n",
    "#     file_path = os.path.join(filterd_abstracts, dir)\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "#     temp_dict = {item['corpusid']: item for item in data}\n",
    "#     all_abstracts.update(temp_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_v1_file794.json\n"
     ]
    }
   ],
   "source": [
    "all_files = []\n",
    "for category in categories:\n",
    "    json_files = list(Path(categories[category]).glob('*.json'))\n",
    "    print(json_files[0].name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import threading\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from util.utils import load_file_line_by_line\n",
    "import logging\n",
    "from threading import Lock\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "class combineFiles:\n",
    "    def __init__(self, categories: dict, main_dict: dict, chunk_size: int, output_dir: str, max_workers = 10):\n",
    "        self.categories = categories\n",
    "        self.main_dict = main_dict\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.output_dir = output_dir\n",
    "        self.max_workers = max_workers\n",
    "\n",
    "    def process_file(self, file_path, lock):\n",
    "        count = 0 \n",
    "        self.logger.info(f\"Processed file: {file_path}\")\n",
    "        for item in load_file_line_by_line(file_path):\n",
    "            corpusid = item['corpusid']\n",
    "            if corpusid in self.main_dict:\n",
    "                count+=1\n",
    "                with lock:\n",
    "                    self.main_dict[corpusid].update(item)\n",
    "        self.logger.info(f\"Finished processing: {file_path.name}. Found {count} matching records\")\n",
    "\n",
    "    def process_category(self):\n",
    "        \"\"\"Process all JSON files within all category folders concurrently.\"\"\"\n",
    "        lock = Lock()\n",
    "        all_files = []\n",
    "\n",
    "        # Collect all JSON files from all categories\n",
    "        for category in self.categories:\n",
    "            json_files = list(Path(self.categories[category]).glob('*.json'))\n",
    "            all_files.extend(json_files)\n",
    "\n",
    "        total_files = len(all_files)\n",
    "        self.logger.info(f\"Total files to process: {total_files}\")\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Submit all files to the executor\n",
    "            futures = {\n",
    "                executor.submit(self.process_file, file_path, lock): file_path\n",
    "                for file_path in all_files\n",
    "            }\n",
    "\n",
    "            # Use tqdm to track the progress of all futures\n",
    "            for future in tqdm(as_completed(futures), total=total_files, desc='Processing all files'):\n",
    "                file_path = futures[future]\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as exc:\n",
    "                    self.logger.error(f'{file_path} generated an exception: {exc}')\n",
    "\n",
    "    def write_chunk_to_file(self, chunk, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(chunk, f)\n",
    "\n",
    "    def create_chunks(self, slice_keys):\n",
    "        \"\"\"Creates chunks from a slice of dictionary keys.\"\"\"\n",
    "        chunks = []\n",
    "        chunk = {}\n",
    "        for key in slice_keys:\n",
    "            chunk[key] = self.main_dict[key]\n",
    "            if len(chunk) >= self.chunk_size:\n",
    "                chunks.append(chunk)\n",
    "                chunk = {}\n",
    "        if chunk:  # Add remaining items\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def parallel_chunking(self):\n",
    "        # Step 1: Split keys into approximately equal slices\n",
    "        keys = list(self.main_dict.keys())\n",
    "        slice_size = max(1, len(keys) // self.max_workers)\n",
    "        key_slices = [keys[i:i + slice_size] for i in range(0, len(keys), slice_size)]\n",
    "        \n",
    "        # Step 2: Use ThreadPoolExecutor to create chunks in parallel\n",
    "        all_chunks = []\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = [executor.submit(self.create_chunks, slice_keys) for slice_keys in key_slices]\n",
    "            \n",
    "            # Collect results from each future\n",
    "            for future in futures:\n",
    "                all_chunks.extend(future.result())\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "    def write_all_chunks(self):\n",
    "        \"\"\"Combines chunking and file writing in parallel.\"\"\"\n",
    "        # Generate all chunks using parallel chunking\n",
    "        chunks = self.parallel_chunking()\n",
    "        \n",
    "        # Write each chunk to a separate file in parallel\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = [\n",
    "                executor.submit(self.write_chunk_to_file, chunk, f\"{self.output_dir}/file_{i + 1}.json\")\n",
    "                for i, chunk in enumerate(chunks)\n",
    "            ]\n",
    "            # Ensure all files are written before exiting\n",
    "            for future in futures:\n",
    "                future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total files to process: 4\n",
      "INFO:__main__:Processed file: /usr/xtmp/hc387/ai_reviewer/data/semantic_scholar/processing_code/temp/file_1.json\n",
      "INFO:__main__:Processed file: /usr/xtmp/hc387/ai_reviewer/data/semantic_scholar/processing_code/temp/file_4.json\n",
      "INFO:__main__:Processed file: /usr/xtmp/hc387/ai_reviewer/data/semantic_scholar/processing_code/temp/file_2.json\n",
      "INFO:__main__:Finished processing: file_1.json. Found 2 matching records\n",
      "Processing all files:   0%|          | 0/4 [00:00<?, ?it/s]INFO:__main__:Processed file: /usr/xtmp/hc387/ai_reviewer/data/semantic_scholar/processing_code/temp/file_3.json\n",
      "INFO:__main__:Finished processing: file_4.json. Found 1 matching records\n",
      "INFO:__main__:Finished processing: file_2.json. Found 2 matching records\n",
      "INFO:__main__:Finished processing: file_3.json. Found 2 matching records\n",
      "Processing all files: 100%|██████████| 4/4 [00:00<00:00, 1033.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{123: {'corpusid': 123, 'abstract': 'this paper is 123', 'check': 'true'}, 456: {'corpusid': 456, 'abstract': 'this paper is 456', 'check': 'true'}, 789: {'corpusid': 789, 'abstract': 'this paper is 789', 'check': 'true'}, 1234: {'corpusid': 1234, 'abstract': 'this paper is 1234', 'check': 'true'}, 12345: {'corpusid': 12345, 'abstract': 'this paper is 12345', 'check': 'true'}, 123445: {'corpusid': 123445, 'abstract': 'this paper is 123445', 'check': 'true'}, 12445: {'corpusid': 12345, 'abstract': 'this paper is 12445'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "temp_main = {\n",
    "    123: {\"corpusid\": 123, \"abstract\": 'this paper is 123'},\n",
    "    456: {\"corpusid\": 456, \"abstract\": 'this paper is 456'},\n",
    "    789: {\"corpusid\": 789, \"abstract\": 'this paper is 789'},\n",
    "    1234: {\"corpusid\": 1234, \"abstract\": 'this paper is 1234'},\n",
    "    12345: {\"corpusid\": 12345, \"abstract\": 'this paper is 12345'},\n",
    "    123445: {\"corpusid\": 123445, \"abstract\": 'this paper is 123445'},\n",
    "    12445: {\"corpusid\": 12345, \"abstract\": 'this paper is 12445'}\n",
    "}\n",
    "categories = {\"temp\": '/usr/xtmp/hc387/ai_reviewer/data/semantic_scholar/processing_code/temp'}\n",
    "output_dir = '/usr/xtmp/hc387/ai_reviewer/data/semantic_scholar/2024_10_8/combined'\n",
    "chunk_size = 2\n",
    "temp = combineFiles(categories = categories, main_dict=temp_main, chunk_size=chunk_size, output_dir = output_dir, max_workers=3)\n",
    "temp.process_category()\n",
    "temp.print_main()\n",
    "temp.write_all_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpusid': 123, 'abstract': 'this paper is 123', 'check': 'true'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
